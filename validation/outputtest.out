2023-12-01 04:58:31.920311: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-12-01 04:58:31.969504: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-01 04:58:32.712534: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. 
The class this function is called from is 'LLaMATokenizer'.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.63it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.15it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.06it/s]
head_wise_activation_length:(11012, 32, 4096)
labels_shape:(11012,)
seperated_head_activation shape:11013
Running fold 0
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:03,  9.88it/s]  6%|▋         | 2/32 [00:00<00:03,  9.42it/s]  9%|▉         | 3/32 [00:00<00:03,  8.76it/s] 12%|█▎        | 4/32 [00:00<00:03,  8.65it/s] 16%|█▌        | 5/32 [00:00<00:03,  8.44it/s] 19%|█▉        | 6/32 [00:00<00:03,  8.35it/s] 22%|██▏       | 7/32 [00:00<00:03,  8.33it/s] 25%|██▌       | 8/32 [00:00<00:02,  8.29it/s] 28%|██▊       | 9/32 [00:01<00:02,  8.18it/s] 31%|███▏      | 10/32 [00:01<00:02,  8.02it/s] 34%|███▍      | 11/32 [00:01<00:02,  8.03it/s] 38%|███▊      | 12/32 [00:01<00:02,  7.99it/s] 41%|████      | 13/32 [00:01<00:02,  7.97it/s] 44%|████▍     | 14/32 [00:01<00:02,  8.00it/s] 47%|████▋     | 15/32 [00:01<00:02,  7.97it/s] 50%|█████     | 16/32 [00:01<00:02,  7.96it/s] 53%|█████▎    | 17/32 [00:02<00:01,  7.93it/s] 56%|█████▋    | 18/32 [00:02<00:01,  7.90it/s] 59%|█████▉    | 19/32 [00:02<00:01,  7.78it/s] 62%|██████▎   | 20/32 [00:02<00:01,  7.73it/s] 66%|██████▌   | 21/32 [00:02<00:01,  7.80it/s] 69%|██████▉   | 22/32 [00:02<00:01,  7.85it/s] 72%|███████▏  | 23/32 [00:02<00:01,  7.84it/s] 75%|███████▌  | 24/32 [00:02<00:01,  7.71it/s] 78%|███████▊  | 25/32 [00:03<00:00,  7.77it/s] 81%|████████▏ | 26/32 [00:03<00:00,  7.72it/s] 84%|████████▍ | 27/32 [00:03<00:00,  7.72it/s] 88%|████████▊ | 28/32 [00:03<00:00,  7.92it/s] 91%|█████████ | 29/32 [00:03<00:00,  7.87it/s] 94%|█████████▍| 30/32 [00:03<00:00,  7.91it/s] 97%|█████████▋| 31/32 [00:03<00:00,  7.90it/s]100%|██████████| 32/32 [00:04<00:00,  7.54it/s]100%|██████████| 32/32 [00:04<00:00,  7.97it/s]
Found cached dataset csv (/home/elicer/.cache/huggingface/datasets/DAMO-NLP-SG___csv/DAMO-NLP-SG--MultiJail-68c839d8239b488e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)
[[0.65625 0.65625 0.65625 ... 0.65625 0.65625 0.65625]
 [0.65625 0.65625 0.65625 ... 0.65625 0.65625 0.65625]
 [0.65625 0.65625 0.65625 ... 0.65625 0.65625 0.65625]
 ...
 [0.78125 0.65625 0.65625 ... 0.65625 0.65625 0.65625]
 [0.6875  0.65625 0.65625 ... 0.65625 0.5     0.65625]
 [0.65625 0.65625 0.6875  ... 0.65625 0.75    0.625  ]]
Heads intervened:  [(7, 14), (8, 6), (8, 19), (9, 5), (9, 9), (9, 22), (10, 2), (10, 5), (10, 8), (10, 15), (11, 22), (12, 30), (13, 19), (15, 10), (15, 15), (15, 16), (16, 13), (16, 17), (17, 13), (17, 26), (17, 29), (18, 27), (19, 0), (19, 6), (19, 31), (20, 8), (20, 12), (20, 23), (20, 26), (21, 14), (21, 23), (23, 10), (23, 17), (23, 22), (23, 29), (24, 15), (24, 23), (25, 10), (25, 14), (25, 25), (26, 9), (26, 13), (27, 22), (29, 0), (30, 14), (30, 26), (31, 26), (31, 30)]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 478.64it/s]
verbose
  0%|          | 0/315 [00:00<?, ?it/s]  0%|          | 0/315 [00:01<?, ?it/s]
layer name:model.layers.7.self_attn.head_out,l2norm(direction * projstd): 0.15185546875
layer name:model.layers.8.self_attn.head_out,l2norm(direction * projstd): 0.22998046875
layer name:model.layers.9.self_attn.head_out,l2norm(direction * projstd): 0.1739501953125
layer name:model.layers.10.self_attn.head_out,l2norm(direction * projstd): 0.247802734375
layer name:model.layers.11.self_attn.head_out,l2norm(direction * projstd): 0.2626953125
layer name:model.layers.12.self_attn.head_out,l2norm(direction * projstd): 0.264404296875
layer name:model.layers.13.self_attn.head_out,l2norm(direction * projstd): 0.2281494140625
layer name:model.layers.15.self_attn.head_out,l2norm(direction * projstd): 0.295166015625
layer name:model.layers.16.self_attn.head_out,l2norm(direction * projstd): 0.461669921875
layer name:model.layers.17.self_attn.head_out,l2norm(direction * projstd): 0.6650390625
layer name:model.layers.18.self_attn.head_out,l2norm(direction * projstd): 0.25830078125
layer name:model.layers.19.self_attn.head_out,l2norm(direction * projstd): 0.6279296875
layer name:model.layers.20.self_attn.head_out,l2norm(direction * projstd): 0.55908203125
layer name:model.layers.21.self_attn.head_out,l2norm(direction * projstd): 0.50390625
layer name:model.layers.23.self_attn.head_out,l2norm(direction * projstd): 1.263671875
layer name:model.layers.24.self_attn.head_out,l2norm(direction * projstd): 0.8544921875
layer name:model.layers.25.self_attn.head_out,l2norm(direction * projstd): 0.66455078125
layer name:model.layers.26.self_attn.head_out,l2norm(direction * projstd): 0.9482421875
layer name:model.layers.27.self_attn.head_out,l2norm(direction * projstd): 0.80419921875
layer name:model.layers.29.self_attn.head_out,l2norm(direction * projstd): 0.305908203125
layer name:model.layers.30.self_attn.head_out,l2norm(direction * projstd): 2.23046875
layer name:model.layers.31.self_attn.head_out,l2norm(direction * projstd): 0.5703125
Running fold 1
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:03,  9.94it/s]  6%|▋         | 2/32 [00:00<00:03,  9.82it/s]  9%|▉         | 3/32 [00:00<00:03,  9.64it/s] 12%|█▎        | 4/32 [00:00<00:02,  9.63it/s] 16%|█▌        | 5/32 [00:00<00:02,  9.49it/s] 19%|█▉        | 6/32 [00:00<00:02,  9.39it/s] 22%|██▏       | 7/32 [00:00<00:02,  9.27it/s] 25%|██▌       | 8/32 [00:00<00:02,  9.16it/s] 28%|██▊       | 9/32 [00:00<00:02,  9.00it/s] 31%|███▏      | 10/32 [00:01<00:02,  8.91it/s] 34%|███▍      | 11/32 [00:01<00:02,  8.84it/s] 38%|███▊      | 12/32 [00:01<00:02,  8.76it/s] 41%|████      | 13/32 [00:01<00:02,  8.72it/s] 44%|████▍     | 14/32 [00:01<00:02,  8.71it/s] 47%|████▋     | 15/32 [00:01<00:01,  8.58it/s] 50%|█████     | 16/32 [00:01<00:01,  8.43it/s] 53%|█████▎    | 17/32 [00:01<00:01,  8.39it/s] 56%|█████▋    | 18/32 [00:02<00:01,  8.38it/s] 59%|█████▉    | 19/32 [00:02<00:01,  8.37it/s] 62%|██████▎   | 20/32 [00:02<00:01,  8.39it/s] 66%|██████▌   | 21/32 [00:02<00:01,  8.45it/s] 69%|██████▉   | 22/32 [00:02<00:01,  8.49it/s] 72%|███████▏  | 23/32 [00:02<00:01,  8.43it/s] 75%|███████▌  | 24/32 [00:02<00:00,  8.33it/s] 78%|███████▊  | 25/32 [00:02<00:00,  8.43it/s] 81%|████████▏ | 26/32 [00:02<00:00,  8.38it/s] 84%|████████▍ | 27/32 [00:03<00:00,  8.38it/s] 88%|████████▊ | 28/32 [00:03<00:00,  8.09it/s] 91%|█████████ | 29/32 [00:03<00:00,  7.99it/s] 94%|█████████▍| 30/32 [00:03<00:00,  7.72it/s] 97%|█████████▋| 31/32 [00:03<00:00,  7.78it/s]100%|██████████| 32/32 [00:03<00:00,  7.48it/s]100%|██████████| 32/32 [00:03<00:00,  8.48it/s]
Found cached dataset csv (/home/elicer/.cache/huggingface/datasets/DAMO-NLP-SG___csv/DAMO-NLP-SG--MultiJail-68c839d8239b488e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)
[[0.875   0.875   0.875   ... 0.875   0.875   0.875  ]
 [0.875   0.875   0.875   ... 0.875   0.875   0.875  ]
 [0.875   0.875   0.875   ... 0.875   0.875   0.875  ]
 ...
 [0.84375 0.875   0.875   ... 0.875   0.875   0.875  ]
 [0.875   0.875   0.875   ... 0.875   0.875   0.875  ]
 [0.875   0.875   0.875   ... 0.875   0.875   0.8125 ]]
Heads intervened:  [(2, 31), (3, 0), (3, 1), (10, 20), (10, 29), (12, 11), (12, 12), (12, 13), (12, 14), (12, 15), (12, 16), (12, 17), (12, 18), (12, 19), (12, 20), (12, 21), (12, 23), (12, 24), (13, 17), (13, 19), (13, 20), (16, 8), (16, 12), (16, 16), (17, 3), (18, 9), (20, 10), (20, 30), (22, 15), (22, 29), (23, 4), (23, 9), (23, 13), (24, 15), (25, 7), (25, 21), (26, 9), (26, 13), (26, 30), (27, 7), (27, 9), (28, 25), (28, 27), (29, 7), (29, 15), (30, 14), (30, 26), (31, 3)]
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 267.82it/s]
verbose
  0%|          | 0/315 [00:00<?, ?it/s]  0%|          | 0/315 [00:00<?, ?it/s]
layer name:model.layers.2.self_attn.head_out,l2norm(direction * projstd): 0.056243896484375
layer name:model.layers.3.self_attn.head_out,l2norm(direction * projstd): 0.049835205078125
layer name:model.layers.10.self_attn.head_out,l2norm(direction * projstd): 0.327880859375
layer name:model.layers.12.self_attn.head_out,l2norm(direction * projstd): 0.1263427734375
layer name:model.layers.13.self_attn.head_out,l2norm(direction * projstd): 0.161865234375
layer name:model.layers.16.self_attn.head_out,l2norm(direction * projstd): 0.330078125
layer name:model.layers.17.self_attn.head_out,l2norm(direction * projstd): 0.160400390625
layer name:model.layers.18.self_attn.head_out,l2norm(direction * projstd): 0.336669921875
layer name:model.layers.20.self_attn.head_out,l2norm(direction * projstd): 1.029296875
layer name:model.layers.22.self_attn.head_out,l2norm(direction * projstd): 0.468994140625
layer name:model.layers.23.self_attn.head_out,l2norm(direction * projstd): 0.270751953125
layer name:model.layers.24.self_attn.head_out,l2norm(direction * projstd): 0.9775390625
layer name:model.layers.25.self_attn.head_out,l2norm(direction * projstd): 0.52978515625
layer name:model.layers.26.self_attn.head_out,l2norm(direction * projstd): 0.496826171875
layer name:model.layers.27.self_attn.head_out,l2norm(direction * projstd): 0.1976318359375
layer name:model.layers.28.self_attn.head_out,l2norm(direction * projstd): 0.5390625
layer name:model.layers.29.self_attn.head_out,l2norm(direction * projstd): 1.38671875
layer name:model.layers.30.self_attn.head_out,l2norm(direction * projstd): 2.294921875
layer name:model.layers.31.self_attn.head_out,l2norm(direction * projstd): 0.46630859375
